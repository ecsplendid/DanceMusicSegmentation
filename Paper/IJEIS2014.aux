\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{Matejka:2013:SIO:2470654.2466149}
\citation{tzanetakis1999framework}
\citation{tzanetakis1999framework}
\citation{eckmann1987recurrence}
\citation{cooper2002automatic,foote1999visualizing,foote1997similarity,foote2000automatic,foote2003media,foote2001visualizing}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Literature Review}{2}{subsection.1.1}}
\citation{foote2003media}
\citation{goodwin2004dynamic}
\citation{goodwin2003audio}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Foote's novelty function for one of the radio shows in the corpus. The actual track indices are shown with dotted lines, and the predicted tracks are shown with the markers. Some of the parameters are drawn from our own method of constructing the self-similarity matrix (see Section\nobreakspace  {}\ref  {sec:feature-extraction})}}{3}{figure.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An example of Foote's Gaussian tapered checkerboard kernel, width 50.}}{3}{figure.2}}
\newlabel{fig:novelty}{{2}{3}{An example of Foote's Gaussian tapered checkerboard kernel, width 50}{figure.2}{}}
\citation{radu}
\citation{elaudio}
\citation{levy2008structural}
\citation{levy2006new,levy2006extraction}
\citation{batlle2002automatic}
\citation{foote2003media}
\citation{peiszer2008automatic}
\citation{elaudio}
\@writefile{toc}{\contentsline {section}{\numberline {2}Corpus}{4}{section.2}}
\newlabel{dataset}{{2}{4}{Corpus}{section.2}{}}
\newlabel{tab:corpus}{{2}{5}{Corpus}{section.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Descriptive statistics about the corpus.}}{5}{table.1}}
\citation{boer1974critical}
\citation{plomp1965tonal}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Track length histogram for all shows in the corpus.}}{6}{figure.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Number of tracks in each show for each dataset. The \texttt  {lindmik} dataset is highly variable.}}{6}{figure.4}}
\newlabel{fig:tracklengths}{{4}{6}{Number of tracks in each show for each dataset. The \texttt {lindmik} dataset is highly variable}{figure.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Human Accuracy}{6}{section.3}}
\newlabel{human_acc}{{3}{6}{Human Accuracy}{section.3}{}}
\citation{nyquist1928certain}
\citation{frigo2004fftw}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Illustration of the `human disagreement' random variable (zoomed in at the bottom, the peak at $0$ seconds is $4708$ tracks), standard deviation $9.13$ seconds. Peaks are visible at intervals of $8$ bars ($14.8$ seconds) which corroborates the analysis from Denis Goncharov in Section\nobreakspace  {}\ref  {dataset}. The $4$ adjacent error clusters account for roughly 5 percent of the total number of tracks. The variance around the peaks represents the BPM variance in \texttt  {asot}. }}{7}{figure.5}}
\newlabel{fig:human_muchconfuse}{{5}{7}{Illustration of the `human disagreement' random variable (zoomed in at the bottom, the peak at $0$ seconds is $4708$ tracks), standard deviation $9.13$ seconds. Peaks are visible at intervals of $8$ bars ($14.8$ seconds) which corroborates the analysis from Denis Goncharov in Section~\ref {dataset}. The $4$ adjacent error clusters account for roughly 5 percent of the total number of tracks. The variance around the peaks represents the BPM variance in \texttt {asot}}{figure.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Data Handling}{7}{section.4}}
\newlabel{sec:data-handling}{{4}{7}{Data Handling}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Preprocessing}{7}{subsection.4.1}}
\newlabel{proprocessing}{{4.1}{7}{Preprocessing}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Feature Extraction}{7}{subsection.4.2}}
\newlabel{sec:feature-extraction}{{4.2}{7}{Feature Extraction}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Music}{7}{subsubsection.4.2.1}}
\newlabel{feat_ex}{{4.2.1}{7}{Music}{subsubsection.4.2.1}{}}
\citation{tzanetakis1999multifeature}
\citation{tzanetakis1999framework}
\citation{foote1999visualizing}
\citation{scarfe2013long}
\citation{scarfe2013long}
\citation{scarfe2013long}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Self-Similarity Matrix}{9}{subsubsection.4.2.2}}
\newlabel{sec:self-similarity}{{4.2.2}{9}{Self-Similarity Matrix}{subsubsection.4.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Illustration of the effect of normalization parameter $\mathaccentV {hat}05Ec=0.7$ on the values in $S$ on radio show \texttt  {asot} $453$. The small raised section on the left correspond to the tracks down the diagonal.}}{9}{figure.6}}
\newlabel{fig:cosine_norm}{{6}{9}{Illustration of the effect of normalization parameter $\hat c=0.7$ on the values in $S$ on radio show \texttt {asot} $453$. The small raised section on the left correspond to the tracks down the diagonal}{figure.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Cost Matrices}{9}{subsubsection.4.2.3}}
\newlabel{sec:cost-matrices}{{4.2.3}{9}{Cost Matrices}{subsubsection.4.2.3}{}}
\newlabel{sec:summation}{{4.2.3}{10}{Summation}{section*.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Summation}{10}{section*.2}}
\@writefile{toc}{\contentsline {paragraph}{Symmetry}{10}{section*.3}}
\newlabel{sec:static-contiguity}{{4.2.3}{11}{Static Contiguity}{section*.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Static Contiguity}{11}{section*.4}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Construct \textit  {contig-static} dissimilarity matrix by modifying $S_{ij}$ in-place.}}{11}{algocf.1}}
\newlabel{algo:contig}{{1}{11}{Static Contiguity}{algocf.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Evolutionary Contiguity}{11}{section*.5}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Construct \textit  {contig-evolution} dissimilarity matrix by modifying $S_{ij}$ in-place.}}{12}{algocf.2}}
\newlabel{algo:contig}{{2}{12}{Evolutionary Contiguity}{algocf.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Gaussian}{12}{section*.6}}
\@writefile{toc}{\contentsline {paragraph}{Mixing Cost Functions}{12}{section*.7}}
\@writefile{toc}{\contentsline {paragraph}{Solution Shift}{12}{section*.8}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Summation cost matrices for Magic Island episode 110 with an incentive bias $\Omega =1$ and therefore containing disincentives. }}{12}{figure.8}}
\newlabel{fig:cmsumib1}{{8}{12}{Summation cost matrices for Magic Island episode 110 with an incentive bias $\Omega =1$ and therefore containing disincentives}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Summation cost matrices for Magic Island episode 110 with an incentive bias $\Omega =0$ and therefore containing incentives. }}{12}{figure.9}}
\newlabel{fig:cmsumib2}{{9}{12}{Summation cost matrices for Magic Island episode 110 with an incentive bias $\Omega =0$ and therefore containing incentives}{figure.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Computing Best Segmentation}{13}{section.5}}
\newlabel{best_cost}{{5}{13}{Computing Best Segmentation}{section.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Confidence Intervals}{14}{section.6}}
\newlabel{sec:confidence-intervals}{{6}{14}{Confidence Intervals}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Posterior Marginal of Song Boundary}{14}{subsection.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Posterior Marginal of Song Position}{14}{subsection.6.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Confidence Measures}{14}{subsection.6.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.1}Index (Order)}{15}{subsubsection.6.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.2}Time}{15}{subsubsection.6.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces A visualization of $log(P(j,s))$ ($\eta =10$) for two of the shows in the training set. Ostensibly; uncertainty pertaining to the correct time and index (i.e. track number $2,3,4$) placement increases towards the middle of the shows.}}{15}{figure.10}}
\newlabel{fig:posterior3}{{10}{15}{A visualization of $log(P(j,s))$ ($\eta =10$) for two of the shows in the training set. Ostensibly; uncertainty pertaining to the correct time and index (i.e. track number $2,3,4$) placement increases towards the middle of the shows}{figure.10}{}}
\citation{scarfe2013long}
\@writefile{toc}{\contentsline {section}{\numberline {7}Experiments}{16}{section.7}}
\newlabel{sec:experiments}{{7}{16}{Experiments}{section.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Training Set}{16}{subsection.7.1}}
\newlabel{sec:training-set}{{7.1}{16}{Training Set}{subsection.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Number Of Tracks Known A Priori}{16}{subsection.7.2}}
\newlabel{sec:apriori}{{7.2}{16}{Number Of Tracks Known A Priori}{subsection.7.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.1}Evaluation}{16}{subsubsection.7.2.1}}
\newlabel{eval_crit}{{7.2.1}{16}{Evaluation}{subsubsection.7.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.2}Finding The Best Parameters}{16}{subsubsection.7.2.2}}
\newlabel{sec:findingbestcostmatrix}{{7.2.2}{16}{Finding The Best Parameters}{subsubsection.7.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.3}Results}{16}{subsubsection.7.2.3}}
\newlabel{sec:results}{{7.2.3}{16}{Results}{subsubsection.7.2.3}{}}
\citation{scarfe2013long}
\citation{scarfe2013long}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The shows randomly selected for inclusion in the \textit  {GitHub training set}.}}{17}{table.2}}
\newlabel{table:githubset}{{2}{17}{The shows randomly selected for inclusion in the \textit {GitHub training set}}{table.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.4}Confidence Interval Analysis}{17}{subsubsection.7.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Number Of Tracks Not Known A Priori}{17}{subsection.7.3}}
\newlabel{sec:trackcount}{{7.3}{17}{Number Of Tracks Not Known A Priori}{subsection.7.3}{}}
\citation{foote2003media}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces These are the main results for all cost matrices with parameters optimized for the best mean absolute accuracy. The tuple $\langle a,b,c \rangle $ is used to indicate the results where $a$ is the median absolute error in seconds, $b$ the mean absolute error in seconds and $c$ the standard deviation in seconds (see Section\nobreakspace  {}\ref  {sec:results}). The experiment number is shown on the left.}}{18}{table.3}}
\newlabel{tab:mean-results}{{3}{18}{These are the main results for all cost matrices with parameters optimized for the best mean absolute accuracy. The tuple $\langle a,b,c \rangle $ is used to indicate the results where $a$ is the median absolute error in seconds, $b$ the mean absolute error in seconds and $c$ the standard deviation in seconds (see Section~\ref {sec:results}). The experiment number is shown on the left}{table.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces These are the main results for all cost matrices with parameters optimized for the best median absolute accuracy.}}{18}{table.4}}
\newlabel{tab:median-results}{{4}{18}{These are the main results for all cost matrices with parameters optimized for the best median absolute accuracy}{table.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Histogram of the residuals (errors) between reconstructed and human captured time indices per experiment (top) and with best mean-optimized mixture broken down by show (bottom). Apart from obvious noise there appears to be a tendency for the algorithm to place an index slightly earlier. }}{19}{figure.11}}
\newlabel{fig:shifthistogram}{{11}{19}{Histogram of the residuals (errors) between reconstructed and human captured time indices per experiment (top) and with best mean-optimized mixture broken down by show (bottom). Apart from obvious noise there appears to be a tendency for the algorithm to place an index slightly earlier}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Comparison between our algorithm and the Foote novelty peak finding approach on all of the datasets.}}{19}{figure.13}}
\newlabel{fig:fscores_best}{{13}{19}{Comparison between our algorithm and the Foote novelty peak finding approach on all of the datasets}{figure.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1}Comparison of Methods For Segmentation}{19}{subsubsection.7.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Confidence intervals and error residuals averaged over show progression.}}{20}{figure.14}}
\newlabel{fig:confidence_intervals}{{14}{20}{Confidence intervals and error residuals averaged over show progression}{figure.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Number of tracks estimated correctly a show in the GitHub training set after a genetics algorithm was executed to select a robust set of algorithm parameters.}}{20}{figure.15}}
\newlabel{fig:github_trackestimation}{{15}{20}{Number of tracks estimated correctly a show in the GitHub training set after a genetics algorithm was executed to select a robust set of algorithm parameters}{figure.15}{}}
\citation{plotz2006automatic}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Track estimation error on our method as described in Section \ref  {sec:trackcount}, Foote's novelty function and na\"{\i }ve guessing. }}{21}{figure.16}}
\newlabel{fig:track_shift}{{16}{21}{Track estimation error on our method as described in Section \ref {sec:trackcount}, Foote's novelty function and na\"{\i }ve guessing}{figure.16}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.2}Results}{21}{subsubsection.7.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{21}{section.8}}
\newlabel{conclusions}{{8}{21}{Conclusion}{section.8}{}}
\citation{scarfe2013long}
\citation{foote2003media}
\citation{radu}
\citation{goodwin2003audio,goodwin2004dynamic}
\bibstyle{ieeetr}
\bibdata{bib/references,bib/refs}
\bibcite{Matejka:2013:SIO:2470654.2466149}{{1}{}{{}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Acknowledgements}{22}{section.9}}
\newlabel{sec:acknowledgements}{{9}{22}{Acknowledgements}{section.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Materials}{22}{section.10}}
\newlabel{sec:materials}{{10}{22}{Materials}{section.10}{}}
\bibcite{tzanetakis1999framework}{{2}{}{{}}{{}}}
\bibcite{eckmann1987recurrence}{{3}{}{{}}{{}}}
\bibcite{cooper2002automatic}{{4}{}{{}}{{}}}
\bibcite{foote1999visualizing}{{5}{}{{}}{{}}}
\bibcite{foote1997similarity}{{6}{}{{}}{{}}}
\bibcite{foote2000automatic}{{7}{}{{}}{{}}}
\bibcite{foote2003media}{{8}{}{{}}{{}}}
\bibcite{foote2001visualizing}{{9}{}{{}}{{}}}
\bibcite{goodwin2004dynamic}{{10}{}{{}}{{}}}
\bibcite{goodwin2003audio}{{11}{}{{}}{{}}}
\bibcite{radu}{{12}{}{{}}{{}}}
\bibcite{elaudio}{{13}{}{{}}{{}}}
\bibcite{levy2008structural}{{14}{}{{}}{{}}}
\bibcite{levy2006new}{{15}{}{{}}{{}}}
\bibcite{levy2006extraction}{{16}{}{{}}{{}}}
\bibcite{batlle2002automatic}{{17}{}{{}}{{}}}
\bibcite{peiszer2008automatic}{{18}{}{{}}{{}}}
\bibcite{boer1974critical}{{19}{}{{}}{{}}}
\bibcite{plomp1965tonal}{{20}{}{{}}{{}}}
\bibcite{nyquist1928certain}{{21}{}{{}}{{}}}
\bibcite{frigo2004fftw}{{22}{}{{}}{{}}}
\bibcite{tzanetakis1999multifeature}{{23}{}{{}}{{}}}
\bibcite{scarfe2013long}{{24}{}{{}}{{}}}
\bibcite{plotz2006automatic}{{25}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces An illustration of the similarity matrix $S$ (cosines) with the actual indices drawn on with black crosses, and our reconstructed annotations indicated with the dotted white lines. Note that to save time on the computation we do not calculate the entire matrix which is why there are some empty regions on the corners.}}{25}{figure.7}}
\newlabel{fig:simmatrix}{{7}{25}{An illustration of the similarity matrix $S$ (cosines) with the actual indices drawn on with black crosses, and our reconstructed annotations indicated with the dotted white lines. Note that to save time on the computation we do not calculate the entire matrix which is why there are some empty regions on the corners}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Comparison of the $F_1$ scores against time thresholds on the $4$ data sets. On the \texttt  {lindmik} dataset where the number of tracks is highly unpredictable, our method combined with track estimation beats Foote's \textit  {enhanced} novelty method at higher thresholds.}}{26}{figure.12}}
\newlabel{fig:fscores_breakdown}{{12}{26}{Comparison of the $F_1$ scores against time thresholds on the $4$ data sets. On the \texttt {lindmik} dataset where the number of tracks is highly unpredictable, our method combined with track estimation beats Foote's \textit {enhanced} novelty method at higher thresholds}{figure.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Results for stochastic optimization (evolutionary algorithm) search of parameter space. Note that the search space $T$ was limited to a minimum of $3$ seconds to save computation time. }}{27}{table.5}}
\newlabel{tab:parameters}{{5}{27}{Results for stochastic optimization (evolutionary algorithm) search of parameter space. Note that the search space $T$ was limited to a minimum of $3$ seconds to save computation time}{table.5}{}}
