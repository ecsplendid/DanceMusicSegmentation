\documentclass[twocolumn]{article}

\usepackage{mathtools}
\usepackage{amsmath,amsfonts}
\usepackage{lscape}
\usepackage{bm}
\usepackage{breqn}
\usepackage{hyperref}
\usepackage{rotating}
\usepackage{pseudocode}

\usepackage{graphicx}
\usepackage[utf8]{inputenc}

\usepackage{tikz}
\usetikzlibrary{shapes.geometric,calc}
\usetikzlibrary{patterns}

\usepackage[square,sort,comma,numbers]{natbib}

\tikzset{
	pro/.style={circle,draw=black,fill=none,inner sep=1pt,minimum size=1.3em},
	sil/.style={circle,draw=none,fill=black,inner sep=0pt,minimum size=5pt}
}

\DeclareBoldMathCommand{\t}{t}
\DeclareMathOperator{\loss}{\ell}
\newcommand{\segs}{\mathbb S}
\newcommand{\head}{\mathcal H}
\newcommand{\tail}{\mathcal T}
\newcommand{\best}{\mathcal V}

\usepackage{multirow}
\usepackage{array}
\usepackage{colortbl}

\newlength\epaisLigne
\newcommand\Mline[2]{\noalign{\global\epaisLigne\arrayrulewidth\global\arrayrulewidth#1pt}\cline{#2}\noalign{\global\arrayrulewidth\epaisLigne}}

%\newcommand{\mynote}[1]{\marginpar{\tiny #1}}
\newcommand{\mynote}[1]{}

\hypersetup{
	pdfauthor={Tim Scarfe, Wouter M. Koolen and Yuri Kalnishkan},
	pdftitle={Segmenting electronic dance music streams based on self-similarity, symmetry, evolution and contiguity, symmetry, evolution and contiguity},
	pdfkeywords={audio,stream,segmentation,dj,mixing},
	pdfsubject={Artificial Intelligence},
	pdfcreator={Tim Scarfe},
	pdfproducer={Tim Scarfe},
}

\author{Tim Scarfe, Wouter M.~Koolen and Yuri Kalnishkan \\ 
	Computer Learning Research
	Centre and Department of Computer Science, \\ 
	Royal Holloway,  University of London, Egham, Surrey, TW20 0EX, United Kingdom\\
	\{tim,wouter,yura\}@cs.rhul.ac.uk
}

\pagestyle{plain}
% to remove in the final version!!


\title{Segmenting electronic dance music streams based on self-similarity, symmetry, evolution and contiguity}
\begin{document}
	
	\maketitle
	
	\begin{abstract}
		
		We describe an unsupervised, deterministic algorithm for segmenting DJ-mixed Electronic Dance Music (EDM) streams (for example; pod-casts, radio shows, live events) into their respective tracks. We attempt to reconstruct a fixed number of boundaries as close as possible to what a human domain expert would assign in respect of the same task. The goal of DJ-mixing is to render track boundaries effectively invisible from the standpoint of human perception which makes the problem difficult.
		
		We optimally segment domain inspired cost matrices derived from a self-similarity matrix. Dynamic programming is used (which means solutions to a problem are described in terms of overlapping sub-solutions to achieve a significant improvement in time complexity and therefore execution time). The similarity matrix is based on the normalized cosines of a time series of transformed Fourier based features. Our method is applied to dance music radio shows. Its formulation incorporates long-term self-similarity over a variable (but limited) horizon and is quantitatively assessed on a large corpus of $304$, two hour long radio shows which have been hand-labelled by a domain expert.
		\smallskip
		
		\noindent \textbf{Keywords.} \noindent music,~segmentation,~DJ,~mix,~dynamic programming
		
	\end{abstract}
	
	
	\vspace{1em}
	
	\section{Introduction}
	
	Electronic Dance Music tracks are usually mixed by a disc jockey (DJ). For this reason EDM music streams are unique compared to other genres of music. Mixing is the \textit{modus operandi} in electronic music. We first transform the audio file into a time series of features discretized into adjacent tiles and transform them into a domain where pairs from the same track would be distinguishable by their cosine. Our features are based on a Fourier transformation with convolution filtering to accentuate prominent instruments and self-similarity within tracks. We create a similarity matrix from these cosines and then derive cost matrices showing the costs of fitting a track at a given time with a given length. We use Dynamic Programming to create the cost matrices and again to perform the most economical segmentation of the cost matrices to fit a predetermined number of tracks.
	
	A distinguishing feature of our algorithm is that it focuses on long term self-similarity of segments rather than transients or a novelty function. Dance music tracks have the property that they are made up of repeating regions, and the heads and tails are often self-similar. For this reason we believe that some techniques from structural analysis fail to perform as well for this segmentation task because we focus on the concept of self-similarity ranging over a configurable time horizon. Our method does not require any training or tenuous heuristics to perform well. 
	
	The purpose of this algorithm is to reconstruct optimal boundaries given a fixed number of tracks known in advance (their names and order are known). This is relevant when one has recorded a show, downloaded a track list and needs to reconstruct the indices given a track list. The order of the indices reconstructed is critical so that we can align the correct track names with the reconstructed indices. If the track list were not known in advance the number of tracks could be estimated quite reliably as the variable of track lengths is Gaussian (see Figure~\ref{fig:tracklengths}).
	
	One of the interesting features of audio is that you \textit{cannot scrub through it, and get an overview in the same way you can with video}. Audio has a reduced \textit{contextual continuum} when the user skips through it, perhaps due to the lack of redundant, persistent scene-setting information or indeed a psychological reason. Even in video applications, discovery, context and scrubbing are an active area of research \cite{Matejka:2013:SIO:2470654.2466149}. Time index meta-data would allow click through monetisation, and allow improved use-case scenarios (for example publishing track names to social networks, information discovery and retrieval). Capturing meta-data in audio is a time consuming and error-prone process. Tzanetakis \cite{tzanetakis1999framework} found that it took users on average $2$ hours to segment $10$ minutes of audio using standard tools. While not directly relevant we might glean from those findings that there is a strong motivation to automate this process.
	
	To mix tracks DJs always match the speed or \textit{BPM} (beats per minute) of each adjacent track during a transition and align the major percussive elements in the time domain. This is the central concept of removing any dissonance from overlapping tracks. Tracks can overlap by any amount. DJs increase adjacent track compatibility further by selecting adjacent pairs that are harmonically compatible and by applying spectral transformations (EQ).
	
	The main theme of the early literature was attempting to generate a novelty function to find points of change using distance-based metrics or statistical methods. Heuristic methods with hard decision boundaries were used to find the best peaks. 
	
	J.\ Foote et al \citep{foote1999visualizing,foote1997similarity,foote2000automatic,foote2003media,foote2001visualizing} have done a significant amount of work in this area and were the first to use similarity matrices. Foote evaluated a Gaussian tapered checker board kernel along the diagonal of a similarity matrix to create a 1-dimensional novelty function. One benefit to our approach is that our DP allows any range of long-term self-similarity (which relates to the fixed kernel size in Foote's work).
	
	Goodwin et al.\ also used DP for segmentation \citep{goodwin2003audio,goodwin2004dynamic}. Their intriguing supervised approach was to perform Linear Discriminant Analysis (LDA) on the features to transform them into a domain where segmentation boundaries would be emphasised and the feature weights normalized. They then reformulated the problem into a clustering DP to find an arbitrary number of clusters. We believe the frame of mind for this work was structural analysis, because it focuses on short term transients (mitigated slightly by the LDA) and would find segments between two regions of long term self-similarity. Goodwin was the first to discuss the shortcomings of novelty peak finding approaches. Goodwin's approach is not optimized to work for a predetermined number of segments and depends on the parametrization and training of the LDA transform. 
	
	Peeters et al \citep{peeters2002toward,peeters2004deriving} did some interesting work combining k-means and a transformation of the segmentation problem into Viterbi (a dynamic program).
	
	% ADD HERE THAT WE ALSO USE LINDMIKS DATASET, so its the true'er' index
	A distinguishing feature of our approach is that we evaluate how well we are doing compared to humans in respect of the same task. We compare our reconstructed indices to the ones created by a human domain expert and the algorithm itself is optimised for the domain of mixed music. 
	
	In the coming sections we describe the corpus (see Section~\ref{dataset}), human accuracy (see Section~\ref{human_acc}),  the evaluation criteria (see Section~\ref{eval_crit}), how we pre-process the data (see Section~\ref{proprocessing}), how we perform feature extraction (see Section~\ref{feat_ex}), designing the cost matrices using observed phenomena in the domain (see Section~\ref{costmatrix_sec}), computing the best segmentation (see Section~\ref{best_cost}), discussion of confidence intervals (see Section~\ref{sec:confidence-intervals}), our methodology (see Section~\ref{sec:methodology}), materials used (see Section~\ref{sec:materials}), results (see Section~\ref{sec:results}) and finally the summary (see Section~\ref{conclusions}).
	
	\section{Corpus}\label{dataset}
	
	We have been supplied with several broadcasts from three popular radio shows. These are: Magic Island, by Roger Shah ($106$ shows); A State of Trance with Armin Van Buuren ($110$ shows); and Trance Around The World with Above and Beyond ($88$ shows) (Total $304$ shows). The show genres are a mix of Progressive Trance, Uplifting Trance and Tech-Trance. We believe this corpus is the largest of its kind used \cite{peiszer2008automatic} in the literature.  
	The music is uninterrupted after the introduction (no silent gaps). 
	The shows come in $44100$ samples per second, $16$ bit stereo MP3 files sampled at $192$Kbs. We resampled these to $4000$Hz $16$ bit mono (left+right channel) WAV files to allow us to process them faster. We have used the ``Sound eXchange''\footnote{\url{http://sox.sourceforge.net}}  program to do this. These shows are all $2$ hours long. The overall average track length is $5$ and a half minutes (slightly less for Magic Island (see Figure~\ref{fig:tracklengths})) and normally distributed. The average number of tracks is $23$ for ASOT and TATW, $19$ for Magic Island. There is a guest mix on the second half of each show. The guest mix DJs show off their skills with technically convoluted mixing, so it is fair to say that the boundary ``complexity'' increases during the guest mix and is at least not fixed throughout the shows.
	
	\begin{figure}
		\centering
		\includegraphics[width=0.45\textwidth]{images/tracklength.pdf}
		
		\caption{The track length distribution for all three radio shows. The \textit{bump} of short tracks (less than $3$ minutes) is often introductions or otherwise extraneous.}
		\label{fig:tracklengths}
	\end{figure} 
	
	There is already a large community of people interested in getting track/time meta-data for DJ sets. ``CueNation''\footnote{\url{http://www.cuenation.com}} is an example of this. CueNation is a website allowing people to submit \textit{cue-sheets} for popular DJ Mixes and radio shows. A cue-sheet is a text file containing time meta-data (indices) for a media file.
	
	We had our time meta-data (indices) and radio shows provided to us and hand captured by \textit{Denis Goncharov}; a domain expert and one of the principal contributors to \textit{CueNation}. One of the significant problems with this task is that there is a random error variable associated with the human captured indices themselves. On some tracks, it is unclear where to place the optimal index and when analysing our results, we have noticed some obvious human errors. Regrettably, there is no clear way of quantifying this. Many of the cue-sheet authors themselves reject the idea of automating the task, citing the poor precision of any such result (they often place indices on the exact MP3 frame). However this sentiment seems misplaced given that they frequently make mistakes or that it is a matter of opinion where to place the track and some consistent method may be preferential. A potential outcome of our method could be an assistance mechanism to help them with initial placements. Our results demonstrate that it is indeed possible to automate this task and that while there is some uncertainty attached to the optimal placement, it is still largely predictable.
	
	\textit{Denis Goncharov} provided us with the following description of how he captures the indices. To quote from a personal email exchange with Denis:
	
	\begin{quote}
		Trance music is made in slices of $8$ bars ($1$ bar is $4$ beats. At $135$ beats per minute, $8$ bars is $\frac{60}{135} ( 4 \cdot 8 ) \approx 14.8$ sec). Trance music tends to be around $130$-$135$ BPM. It is a matter of personal preference which point of the transition to call the index. My preference is to consider the index to be the point at which the second track becomes the focus of attention and the first track is sent to the background. Most of the time the index is the point at which the bass line ($400$Hz and lower) of the previous track is cut and the bass line of the second track is introduced. If the DJ decides to exchange the adjacent tracks gradually over the time instead of mixing them abruptly then it is up to the cue-sheet maker to listen further into the second track noting the musical qualities of both tracks and then go back and choose at which point the second track actually becomes the focus of attention.
	\end{quote}
	
	The most obvious and pervasive element in dance music is the percussion (the beats). We believe on balance that ignoring the percussive information is advantageous, because DJs use percussion primarily to blur boundaries between tracks.  We tried to capture percussive based features and found that the transitions between tracks and indeed groups of tracks appeared as stronger self-similar regions than the actual tracks. The percussive feature extractor transformed the autocorrelation of the audio samples in the time domain tiles, and compared the cosine of their absolute values. It was reasonably clear from that  research that track boundaries are revealed with less uncertainty between instruments and harmonic content. However. We do not rule out looking at percussive features again in future research because we are currently ignoring potentially useful features.
	
	Some DJs ``mix harmonically'' (by matching instruments as opposed to percussion) but this preys on human hearing and perception. An algorithm capturing the harmonic information would most likely be able to distinguish two harmonically compatible tracks. 
	
	\section{Human Accuracy}\label{human_acc}
	
	We did some analysis on how accurate the humans themselves are at creating indices. In the absence of a perfect data set our analysis instead hinged on the amount to which the humans disagreed with each other aggregated over a large amount of historical data. Mikael Lindgren was kind enough to send us a dump of his cuesheet database to experiment with. As ASOT is such a popular show there were many independently captured cuesheets to compare against for all of the historical shows. We selected all the shows having at least $3$ distinct cuesheets (not copies or shifted/misaligned copies of each other)  and such that all the cuesheets had the same number of tracks. The first track was ignored (as it was always $0$ seconds). We ended up with $115$ shows with $3$ authors. $65$ shows with $4$ authors and $30$ shows with $5$ authors.  We generated a histogram of distances from the median time for each track, for each cuesheet and assumed values greater than $100$ seconds or less than $-100$ seconds were outliers. The standard deviation of the `human disagreement' variable is $9.13$ seconds. See Figure~\ref{fig:human_muchconfuse} for an illustration. So at this stage it does not seem feasible for us to achieve a higher accuracy when we are evaluating against a method which is intrinsically error prone.
	
	\begin{figure}
		\centering
		\includegraphics[width=0.45\textwidth]{images/human_confusion}
		
		\caption{Illustration of the `human disagreement' random variable (zoomed in at the bottom), standard deviation $9.13$ seconds. Peaks are visible at intervals of $8$ bars ($\approx 14.8$ seconds) which corroborates the analysis from Denis Goncharov in Section~\ref{dataset}. }
		\label{fig:human_muchconfuse}
	\end{figure} 
	
	   
	
	\section{Evaluation Criteria}\label{eval_crit}
	
	It is challenging to quantify the performance of our method because if we misplace any tracks, it may have a cascade effect. For example if we place one track too many early on in a show, many of the subsequent tracks may be correctly detected but placed out of alignment. 
	
	We perform three types of evaluation: `\textit{average}', \textit{heuristic precision}', and `\textit{thresholds}'.
	
	The \textit{average} (in seconds) given as $$\hat A(P,A) = \frac{1}{|P|}\sum_{i=1}^{|P|}{|P_i-A_i|}$$ is the mean absolute difference between constructed and actual indices ($P$ is constructed indices, and $A$ is the human indices). Naturally this metric will have a reduced meaning in some sense because although it depends on the real accuracy it also depends on the number of misplacements and where in the track those misplacements were. The uncertainty variable attached to the misplacements is wide (in relation to the size of the shows and the number of tracks) but does regularise over the course of a large dataset. This metric is the most meaningful for capturing the overall \textit{robustness} of the method.
	
	The \textit{heuristic precision} metric denoted by 
	\[
	\hat H(P,A) =  \mathrm{Median}\left(    \mathrm{Sort}\left( |P_i-A_{1,2,\ldots,|P|}|\right)_1 \right)
	\] for all $i=1,2,\ldots,|A|$
	takes the median of the absolute differences between each prediction in $P$ and the nearest actual index in $A$. This is the most meaningful single metric of \textit{accuracy}. It is largely invariant to misplacements.
	
	The thresholds metric is the percentage of matched tracks within different intervals of time \[\{~60, ~30, ~20, ~10, ~5, ~3, ~1~\},\] in \emph{seconds} as a margin around any of the track indices we have been given. This metric is also invariant to alignment and is provided for comparison with our other paper \cite{scarfe2013long}.
	
	Note that our results will contain the mean average of these metrics (captured for each show) across the dataset being evaluated. 
	
	\section{Preprocessing}\label{proprocessing} % Fourier/xcorr
	
	The dataset had some outliers that may have slightly distorted the analysis of our method. Many of the ``tracks'' in our data set (of indices) were in fact not tracks at all but rather introductions or voice-overs. Almost all of these outlier tracks were short in length. These are quite clearly visible on the distribution of track lengths on Figure~\ref{fig:tracklengths}. To ameliorate the situation we removed any tracks that were shorter than $180$ seconds. We also removed any end tracks that were shorter than $240$ seconds as very often the end tracks on a radio show contain strange elements (for example voice-overs, interviews, show-related `jingles'). This required some manipulation of the cue-sheets and audio files. The undesired segments of the audio files were chopped out, and the cue-sheets were re-flowed so that the time indices point to the correct location in the file. 
	
	The algorithm still performs similarly when removing just these indices and leaving the audio intact underneath, so it would not significantly affect any real-world implementation. 
	
	For those wishing to use this algorithm in practice with pre-recorded shows; the introductions at the start of the shows are often fixed length or at least predictable so error would be small on average.
	
	\section{Feature Extraction}\label{feat_ex} % Fourier/xcorr
	
	We used SoX (see Sect.~\ref{dataset}) to downsample the shows to $4000$Hz. We are not particularly interested in frequencies above around and above $2000$Hz because instrument harmonics become less visible in the spectrum as the frequency increases. The Nyquist theorem \cite{nyquist1928certain} states that the highest representable frequency is half the sampling rate, so this explains our reason to use $4000$Hz. We will refer to the sample rate as $R$. Let $L$ be the length of the show in samples.
	
	Fourier analysis allows one to represent a time domain process as a set of integer oscillations of trigonometric functions. We transform the tiles into the frequency domain using the discrete Fourier transform
\[
F(x_k) = X_k = \sum_{n=0}^{N-1} x_n \cdot e^{-i 2 \pi \frac{k}{N} n}
\] which transforms a sequence of complex numbers $x_0,\ldots,x_N$ into another sequence of complex numbers $X_0,\ldots,X_N$ where $$e^{-i 2 \pi \frac{k}{N} n}$$ are points on the complex unit circle. Note that the fftw algorithm \cite{frigo2004fftw} that we used to perform this computation operates significantly faster when $N$ is a power of $2$ so we zero pad the input to the next power of $2$. We denote the tile width by $M$ in seconds (an algorithm parameter). Note that
\[
N = \frac{L}{M}
\] denotes the tile size in samples (length of show in samples over the tile size). Let 
\[
T = \left\lfloor\frac{L}{\tilde{M}}\right\rfloor
\] be the total number of tiles, and \[
\tilde{M}=\frac{L}{N}
\] the tile width in samples.
Because we are passing real values into the $F(x_k)$, the second half of the result is a rotational copy of the first half.
	
	Show samples are collated into a time series $Q_i^y$ ($T \times N$) of contiguous, non-overlapping, adjacent \emph{tiles} of equal size where $i=1,2,\ldots, T$. Samples at the end of the show that do not fill a complete tile get discarded. The affect of this is increasingly negligible with decreasing tile size. Since we zero-pad $N$ to the next power of two, this also decreases the affect.
	
	As we are not always interested in the entire range of the spectrum, we use $l$ to represent a low pass filter (in Hz) and $h$ the high pass filter (in Hz). So we will capture the range from $h$ to $l$ on the first half of the result of $F$. Let $\hat{h} = \lceil~h~\cdot~\frac{{N}}{R} ~\rceil+1$ be the position of $h$ in the spectrum, and $\hat l=~\lceil~l~\cdot~\frac{{N}}{R}~\rceil+1$ be the position of $l$ in the spectrum.
	
	Let $D_e^y$ ($T \times \hat{l}-\hat{h}+1$) denote the feature matrix.
	
	For each tile $\bar{i}=1,2,\ldots,T$ we assign 
\[
D_{\bar{i}}^{1,\ldots,\hat{l}-\hat{h}+1} = \left|~F(Q_{\bar{i}}^{1,\ldots,\tilde{M}})_{\hat{h},~\hat{h}+1,~\ldots,~\hat{l} }~\right|
\]
 selecting the part of the spectrum between the high and low pass filters $h$ and $f$.  We take the absolute values of the complex result of $F(x_k)$ (defined as its distance in the complex plane from the origin using the Pythagorean theorem).
	
	To accentuate instrument harmonics we perform convolution filtering on the feature vectors in $D$, using a Gaussian first derivative filter. This works like an edge detection filter but also expands the width of the transients (instrument harmonics) to ensure that feature vectors from the same song appear similar because their harmonics are aligned on any distance measure (we use the cosines). This is an issue because of the extremely high frequency resolution from having such large inputs into $F(t_i)$. For example with a tile size of $10$ seconds and a sample rate of $4000$ we have a frequency resolution of $\frac{1}{2}10 \cdot 4000 = 20$KHz. 
	
	Typically a `short-time discrete Fourier transform' is used which has smaller sized inputs (windows) into $F(t_i)$ which are usually overlapping and are multiplied by a window function, attenuating the tails to reduce spectral leakage. Usually these window functions look similar to a Gaussian, for example;  
	\[
	\mathrm{Hann}_i = 0.5 - 0.5 \cos\frac{2\pi i}{n-1}{w(i)}
	\] where $n$ is the window size
	(see \cite{tzanetakis1999multifeature} for an example). The short-time Fourier transform is relevant when increased time precision is needed as there is a frequency-time resolution trade-off with respect of the input size to $F(t_i)$. This is not a concern in this particular application as our time resolution is never required to be better than $1$ second which would still produce adequate frequency resolution.
	
	The Gaussian first derivative filter is defined as 
	$$- \frac{2 \hat{\lambda}}{\upsilon^2}  e^{-\frac{\hat{\lambda}^2}{\upsilon^2}} $$ where 
	
	$$\hat{\lambda}=\left\{~-\lfloor 2\upsilon\rfloor,\lfloor-2\upsilon+1\rfloor,\ldots, \lfloor 2 \upsilon \rfloor~\right\},$$ and $$\upsilon = b  \frac{N}{R} .$$ $b$ is the bandwidth of the filter in Hz and this is a parameter of the algorithm. After the convolution filter is applied to each feature vector in $D$, we take the absolute values and normalize on the vector lengths.  
	
	Because the application domain is well defined in this setting, we can design features that look specifically for what we are interested in (musical instruments). Typically in the literature; algorithms use an amalgam of general purpose feature extractors. For example; spectral centroid, spectral moments, pitch, harmonicity \cite{tzanetakis1999framework}. We construct a dissimilarity matrix of cosines as is common in the literature for similar applications \cite{foote1999visualizing}. The cosines are computable easily because they are the the inner products of the respective features (the features have been normalized to unit length).
	
	Let
	\[
	S_{T \times T} ~=~ 1-\left<~D_{i} \cdot D_{j}~\right>,
	\] define the dissimilarity matrix.
	
	We center $S$ around its mean \[
	S = S^{2 \cdot \frac{1}{T^2}\sum_{i=1}^T \sum_{j=1}^T S_{ij}},
	\] then normalize it by $\hat c$ (an algorithm parameter), $S=S^{\hat c}$ and place it on a small interval around $0$. $S_{ij} = \left( 2 \cdot S_{ij} \right)-1$. It is necessary to have \textit{incentives} and \textit{disincentives} for meaningful track placement which is why we opted for this interval. The distribution of values in $S$ is normal with a raised section on left (negatives) representing the tracks we want to find. When $\hat c$ is $1$ there is no change, smaller values will shift the mean above $0$ and vice versa. This will become relevant when we discuss cost matrices as some of them depend on the sign of the value in $S$. 
	
	See Figure~\ref{fig:simmatrix} for an illustration of $S$ and Figure~\ref{fig:cosine_norm} for an illustration of the cosine normalization.
	
		\begin{figure}
			\centering
			\includegraphics[width=0.45\textwidth]{images/cosinesmagic110}
			
			\caption{Illustration of the effect of normalization parameter $\hat c=0.7$ on the values in $S$ on radio show Magic Island $110$. The small raised section on the left correspond to the tracks.}
			\label{fig:cosine_norm}
		\end{figure} 
	
	\begin{figure*}[t]
		\centering
		\includegraphics[width=1.08\textwidth]{images/S}
		\caption{An illustration of the similarity matrix $S$ with the actual indices drawn on with white crosses, and our reconstructed indices indicated with the black dotted lines. There are examples here of evolutionary repetition ($t=500,\ldots,550$), static contiguity everywhere where there is solid black, and symmetry on the middle two tracks.}
		\label{fig:simmatrix}
	\end{figure*} 
	
	\section{Cost Matrices}\label{costmatrix_sec} % dynamic recursion
	
	We now have a similarity matrix $S(i,j)$ as described in Section~\ref{feat_ex}. 
	
	Let $w$ and $W$ denote the minimum and maximum track length in seconds, these will be parameters.
	
	We require a cost matrix $C(f,t)$ that describes the cost of placing a track of length $t-f+1$ at $f$. After analysing the data set, we have created $7$ cost matrices that exploit observed phenomena in $S$. We also provide an additional cost matrix which is just a Gaussian random function centred around the mean track length for all times which can be used to regularise the other $7$ matrices or used on its own as a comparator to a more naive method of placement. 
	
	The more sophisticated matrices exploit themes such as contiguity, symmetry, evolution and change as well as simple summation of $S$ as was presented in our last paper. In our previous work \cite{scarfe2013long} $S$ was on the interval $[0,1]$ and the summation method could only consider disincentives. All of the new matrices have a parameter to shift the consideration of incentive versus disincentive and will contain values on the interval $[-1,1]$.
	
	Intuitively, most tiles within the same track are similar, while pairs of tiles that do not belong to the same track are significantly more dissimilar. However, often this is not the case and many tracks have dissimilar regions within them. 
	
	\subsection{Normalization}
	
	We define $N(C,\Omega)$ as a normalization function for cost matrix $C$ and incentive bias $\Omega$. It will return a $[-1,1]$ interval when $\Omega=0.5$, $[-1,0]$ when $\Omega=1$  and $[0, 1]$ when $\Omega=0$ and will produce a convex interpolation for other values.
	
	Let 
	\begin{dmath*}
		\hat N(C,\Omega) = \left( \frac{ C_{ij} - \min \left( C_{ij} \right) }{ \max \left( C_{ij} \right) - \min \left( C_{ij} \right) } \cdot \hat h  \right)-s
	\end{dmath*}
	for all $i,j \in C$ where $\hat h$ = $2-\left( 2\cdot|0.5-\Omega| \right)$ and 

	    \begin{dmath*}
	    	s= { \left\{
	    		\begin{array}{ll}
	    			1-2 \cdot |0.5-\Omega| & \mbox{~if~}\Omega > 0.5   \\
	    			1 & \mbox{~otherwise~}.  \\
	    		\end{array}
	    		\right.}
	    \end{dmath*}
	
	Note that to save space we will use the following notation $\hat N(C, \Omega)=\hat N_\Omega(C)$.

	\subsection{Summation}
	
	The most obvious strategy of all is to sum up all relevant tiles in $S$ for each candidate track from tile $f$ through tile $t$. We define $C(f,t)$, the cost of a candidate track from tile $f$ through tile $t$, to be the sum of the similarities between all pairs of tiles inside it, normalized on track length.
	
	\begin{dmath*}
		C(f,t, \Omega) ~=~ \frac{ \sum_{i=f}^{t} \sum_{j=f}^{t} \hat S_\Omega(i,j) }{ (t-f+1) }
	\end{dmath*}
	
	where
	\begin{dmath*}
		\hat S_\Omega=	{ \left\{
			\begin{array}{ll}
				\Omega S_{ij}  & :  ~S_{ij} > 1   \\
				 \left( 1-\Omega \right) S_{ij} & : \mbox{~otherwise~}\\
			\end{array}
			\right.} 
	\end{dmath*} for all $i,j \in S.$ Direct computation using the definition takes $O(TW^3)$ time. We can improve this to $O(TW^2)$ using the recursive formulation for the unnormalized quantity 
	\begin{dmath*} 
		\begin{array}{ll}
		\tilde C( t-(w+1),t+(w-1) ) = \\
		 \tilde C( t-(w-1),t+(w-1) ) \\
		\hspace{1em} +  \sum_{i=1}^{w} \tilde C(t-(w-1),t+(w-1)+i)
		\end{array}
	\end{dmath*} for all $t=1,\ldots, T$ and $w=1,\ldots,\min(w,t)$.
	
	$O(TW)$ is achievable using the following recursion for the unnormalized quantity $\tilde
	C(f,t) = C(f,t)(t-f)$ (for $f+1 \le t-1$)
	\begin{dmath*}
		\tilde C(f,t) ~=~ \tilde C(f+1,t) + \tilde C(f,t-1) - \tilde C(f+1,t-1) + \hat S(f,t) + \hat S(t,f).
	\end{dmath*} (having first pre-computed $\tilde{C}$ for each $1 \le f \le t \le T$). This can be better understood with the following illustration (showing calculations of $\tilde{C}(f,t)$ in respect of the domain of $S$, where the middle cell $\Upsilon = \tilde{C}(f+1,t-1)$)
	
	\begin{center}
		\begin{tikzpicture}
		\draw[step=1cm,color=gray] (0,0) grid (3,3);
		
		\draw[fill=gray!90, fill opacity=0.8] (1,1) -- (1,2) -- (2,2) -- (2,1) -- (1,1);
		\draw[fill=gray!10, fill opacity=1] (0,2) -- (0,3) -- (1,3) -- (1,2) -- (0,2);
		\draw[fill=black!10, fill opacity=0.8] (2,0) -- (3,0) -- (3,1) -- (2,1) -- (2,0);
		
		
	\draw [step=0.5cm, pattern=north east lines] (0,0) rectangle (2,2);
	
		
		\node[color=black] at (0.5,2.5) {$\hat S_\Omega^{(f,t)}$};
		\node[color=black] at (2.5,0.5) {$\hat S_\Omega^{(f,t)}$};
		
		
		\draw[color=black] (0,0) -- (0,2) -- (2,2) -- (2,0) -- (0,0);
		\draw[color=black] (3,3) -- (1,3) -- (1,1) -- (3,1) -- (3,3);
		
		\node[color=black,fill=white, fill opacity=0.8] at (1,1) {$\tilde{C}(f,t-1)$};
		\node[color=black,fill=white, fill opacity=0.8] at (2,2) {$\tilde{C}(f+1,t)$};
		
		\node[color=white] at (1.5,1.5) {$\Upsilon$};
		
		\end{tikzpicture}.
	\end{center} 
	Clearly, the solutions of $\tilde{C}$ overlap in $S$.
	The final cost matrix is normalized by width and incentive bias
\[
C = \hat N_\Omega \left( \frac{ \tilde C_{ft} }{ t-f+1 } \right)
\] for all $t,f \in \tilde C$.
	
	\subsection{Symmetry}	
	
	A common feature on dance music tracks is partial mirror-symmetry.
	
	Let \begin{dmath*}
		\Lambda( f,t,d ) = \left\{~ 
		S(f+d-1,f), S(f+d, f+1), S(f+d+1, f+2),\ldots, S(t, t-d-1)
		~ \right\} 
	\end{dmath*}take the diagonal $d$ from track 
	
	\[
	\hat t= S(f,f+1,\ldots,t,f,f+1,\ldots,t).
	\] For each diagonal in one triangle/half of $\hat t$ (it is a symmetric matrix) we want to compare each element against its mirror counterpart. `Symmetric' pairs that contain the same sign are multiplied together. Pairs that have a different sign are set to $0$.

	Let the symmetry cost matrix
	\begin{dmath*} C = \frac{\hat N_\Omega(~ \tilde C(f, t, \Omega, \delta)~)}{(t-f+1)}\end{dmath*} using the following definition for $\tilde C$
	\begin{dmath*}\tilde C=	\sum_{d=1}^{t-f+1} 
		\sum_{i=1}^{|\Lambda( f,t,d )|} \tilde{s}(\Lambda( f,t,d )_i,\Lambda( f,t,d )_{ |\Lambda( f,t,d )|-i-1 })
	\end{dmath*}  where
\begin{dmath*}
	\tilde{s}(p,q) = 
	\left\{
	\begin{array}{lr}
		0  & : ~\mathrm{sign}( p ) \ne \mathrm{sign}( q )   \\
		\delta( p, q, \Omega ) & : \mbox{~otherwise.}  \\
	\end{array}
	\right.
\end{dmath*}
	We will consider $3$ types of symmetry cost matrix. The \textit{standard symmetry} version where pair evaluator 
		\begin{dmath*}\delta( p,q, \Omega ) = \left\{
				\begin{array}{lr}
					(\Omega) ( p \cdot q )  & : \max(\mathrm{sign}(p),\mathrm{sign}(q))=1,   \\
					 (1-\Omega) ( p \cdot q ) & :~~~\mbox{~otherwise} \\
				\end{array}
				\right.
		\end{dmath*}
		
		and \textit{symmetry summation} is
		\begin{dmath*}\hat \delta( p,q, \Omega ) = \left\{
			\begin{array}{lr}
			(\Omega) \frac{1}{2} ( p + q )	  & :  \max(\mathrm{sign}(p),\mathrm{sign}(q))=1   \\
			 (1-\Omega) \frac{1}{2} ( p + q ) & : \mbox{~otherwise.}	 \\

			\end{array}
			\right.
	\end{dmath*}
	
	Finally, the symmetry change matrix is defined where $\hat \delta$ is the same as the \textit{symmetry summation}, the width normalization is removed and the inner sum of the definition becomes the first order differences. For the sake of brevity we have omitted the dynamic programming formulations which run in $O(TW^2)$ time but they have been implemented in code and are available on-line (see Section~\ref{sec:materials}). This is also the case for all the following cost matrices.
	
	\subsection{Static Contiguity}
	
	Horizontal contiguous traces in $S$ indicate that the track is self-similar (low value $\le 0$) or self-dissimilar ($\ge 0$) due to repetition with the additional information that it is not evolving with respect of time either. If a given tile is the same as a set of contiguous tiles following it, we can assume that there is some static contiguous region. We define contiguity parameter $\rho$ to indicate how many contiguous tiles are required. Let 
\begin{align*}
\begin{array}{ll}
\Gamma( f,t,h ) &= S(~ f + h - 1,~ \tilde{i}(f, t, h) ~)
\end{array}
\end{align*} define the future self-similarity trace for track between $f$ and $f+t-1$ and from time horizon $h$ where 
\begin{dmath*}
\tilde{i}(~f,t,h~) =\\ \{~ f + h - 1, f + h, \ldots, t-h-1 ~ \}.
\end{dmath*}
We define the future contiguity quantity $C$
	
\begin{dmath*}  C(f, t)  =  
\hat N_\Omega \left( \frac{ \tilde	\sum_{h=1}^{t-f+1} 
	\sum_{i=\rho}^{|\Gamma( f,t,h )|} \tilde{x}\left( \Gamma( f,t,i )_{\tilde{h}} \right) }{t-f+1}  \right)
\end{dmath*}  where  

\begin{dmath*}
\tilde{h} = \{i-\rho,i-\rho+1, \ldots,i \},
\end{dmath*}
\begin{dmath*}
\tilde{x}(v) = { \left\{ 
	\begin{array}{lr}
		  \tilde \delta( v, \Omega ) & : \mathrm{range}( \tilde{n}(v) ) = 0  \\
		0 & : \mbox{~otherwise,} \\
	\end{array}
	\right.}
\end{dmath*}
\begin{dmath*}
	\tilde{n}(v) = \{~ sign(v_1), sign(v_2),\ldots, sign(v_{|v|}) ~\},
\end{dmath*}
\begin{dmath*}\tilde \delta( v, \Omega ) = \frac{1}{|v|} \sum^{|v|}_{x=1}
		\tilde{f}(\Omega, v) 
\end{dmath*}	
	and
	\begin{dmath*}
	\tilde{f}(\Omega, v) = \left\{ 
	\begin{array}{lr}
		\Omega \cdot v_x &:  ~\mathrm{sign}(v_1)=1 \\
		(1-\Omega) \cdot v_x &: \mbox{~otherwise.}    \\
	\end{array}
	\right.
\end{dmath*}
	We will also create a past contiguity cost matrix where we replace $\Gamma( f,t,h )$ with a version 

\begin{align*}
\begin{array}{ll}
\hat{\Gamma}( f,t,h ) &= S(~\hat{i}(f, t, h), f + h - 1~),\\
\end{array}
\end{align*}
where
\begin{dmath*}
	\hat{i}(~f, t, h~) =\\ \{~ t - h - 1, t - h, \ldots, f+h-1 ) ~\}
\end{dmath*}

	which traces past self-similarity from the end of the track being fitted.
	
	\subsection{Evolutionary Contiguity}
	
	Any diagonal traces in $S$ that are parallel to the main diagonal are partial copies of the track in the future which evolve in respect of time. Evolutionary Contiguity is a cost matrix which compares all adjacent pairs on the diagonals in $\hat t$, using the comparator $\delta( p,q, \Omega )$ from the standard symmetry cost function and multiplies those values by the time horizon. All the values are summed and normalized by the track width squared. 
	
	Let the evolutionary cost matrix
	\begin{dmath*} C(f, t, \Omega) =  
		\hat N_\Omega\left(\frac{
		\sum_{d=1}^{t-f+1} 
		\sum_{i=2}^{|\Lambda( f,t,d )|}  \delta( \kappa_{i}, \kappa_{i-1}, \Omega ) \cdot d
	}{(t-f+1)^2}\right)
	\end{dmath*}  where  $\kappa_i=\Lambda( f,t,i )$. As before $\tilde C$ is normalized by track width and incentive bias.
	
	\subsection{Gaussian}
	Let \[
	G( \varpi, N )_{tw} = e^{ - \frac{1}{2} \frac{\varpi n}{ \frac{1}{2} W}^2  }
	\] for all $n=1,2,\ldots,W$ denote the Gaussian matrix cost function of $N\times W$. $G( \varpi, N )$ is time-independent and every row is the same. We will use this cost function for regularising the others and for use on its own for comparison against a `naive' competitor. Increasing values of $\varpi$ will tighten up the Gaussian although after experimentation we observed that $1$ was always the best value and stuck with that.
	
	\subsection{Mixing Cost Functions}
	
	We mix cost matrices together by adding them. In our experiments we will have a parameter for each cost matrix $\in [0,1]$ to show its contribution to the mixture. The cost matrices will be multiplied by this number before being mixed. 
	
	See Figure~\ref{fig:cost_matrices} for an illustration of a single cost matrix and a mixture. 

	\begin{figure*}[t]
		\centering
		\includegraphics[width=0.45\textwidth]{images/cm5}
		\includegraphics[width=0.45\textwidth]{images/cm10}
		\caption{Cost matrices for Magic Island episode 110. On the left is the cost matrix parameters from experiment 5 (summation only with incentive bias $0.5$) and the right the mixed cost matrix using parameters from experiment $10$. On the right, the predicted (white) and actual tracks (black) are shown in place. Note that the white space is infinite corresponding to the minimum and maximum track parameters $w$ and $W$.}
		\label{fig:cost_matrices}
	\end{figure*} 
	
		\begin{figure*}[t]
			\centering
			\includegraphics[width=0.45\textwidth]{images/cmsumib1}
			\includegraphics[width=0.45\textwidth]{images/cmsumib0}
			\caption{Summation cost matrices for Magic Island episode 110. The matrix on the left has an incentive bias $\Omega=1$ and therefore only contains disincentives. However the matrix on the right is the other extreme, $\Omega=1$ containing only incentives. }
			\label{fig:cmsumib1}
		\end{figure*} 
	
	
	\section{Computing Best Segmentation}\label{best_cost}
	
	We obtain the cost of a full segmentation by summing the costs of its tracks. The goal is now to efficiently compute the segmentation of least cost.
	
	We want to reconstruct $m$ track boundaries ($m+1$ tracks).
	
	A sequence $\t = (t_1, \ldots, t_{m+1})$ is called an $m/T$-segmentation if and only if
	\[
	1 = t_1 < \ldots < t_m < t_{m+1} = T+1.
	\]
	$m$ is the number of tracks we are trying to find and is a parameter of the algorithm. We use the interpretation that track $i \in \{1, \ldots, m\}$ comprises times $\{t_i, \ldots, t_{i+1}-1\}$. Let $\segs^T_m$ be the set of all $m/T$-segmentations. Note that there are a very large number of possible segmentations 
	\begin{multline*}
		|\segs^T_m| ~=~ \binom{T-1}{m-1}
		= \frac{(T-1)!}{(m-1)!(T-m)!} =\\
		\frac{(T-1)(T-2)\cdots(T-m+1)}{(m-1)!} \ge \left( \frac{T}{m}\right)^{m-1}.
	\end{multline*} 
	For large values of $T$, considering all possible segmentations using brute force is infeasible. For example, a two hour long show with $25$ tracks would have more than 
	\[
	\left( \frac{60^2 \times 2}{25}\right)^{24}  \approx 1.06 \cdot 10^{59}
	\] possible segmentations. 
	
	We can reduce this number slightly by imposing upper and lower bounds on the song length.  Recall that $W$ is the upper bound (in seconds) of the song length, $w$ the lower bound (in seconds) and $m$ the number of tracks. With the track length restriction in place, the number of possible segmentations is still massive. A number now on the order of $10^{56}$ for a two hour show with $25$ tracks, $w=190$ and $W=60\cdot15$.
	
	Let $N(T,W,w,m)$ be the number of segmentations with time $T$ (in tiles),
	
	We can write the recursive relation $$N(T,W,w,m) = \sum N(t_m-1,W,w,m-1),$$ where the sum is taken over $t_m$ such that 
	\begin{align*}
		t_m &\le T-w+1 & t_m &\ge T-W+1\\
		t_m &\ge (m-1)w+1 & t_m &\le (m-1)W+1
	\end{align*}
	
	The first two inequalities mean that the length of the last track is within an acceptable boundary between $w$ and $W$. The last two inequalities mean that the lengths of the first $m-1$ tracks are within the same boundaries. 
	
	We calculated the value of $N(7000,60\cdot15,190,25)$ and got $5.20 \cdot 10^{56}$ which is still infeasible to compute with brute force.
	
	Our solution to this problem is to find a dynamic programming recursion.
	
	The loss of an $m/T$-segmentation $\t$ is 
	\[
	\loss(\t) 
	~=~
	\sum_{i=1}^m C(t_i, t_{i+1}-1)
	\]
	We want to compute
	\[
	\best^T_m ~=~ \min_{\t \in \segs^T_m} \loss(\t)
	\]
	To this end, we write the recurrence
	\begin{equation*}
		\best^t_1 ~=~ C(1, t) 
	\end{equation*}
	and for $i\ge2$
\begin{align*}
		\best^t_i ~=~& \min_{\t \in \segs^t_i} \loss(\t)  \\
		~=~& \min_{t_i} \min_{\t \in \segs^{t_i-1}_{i-1}} \loss(\t) + C(t_i, t)  \\
		~=~&  	\min_{t_i} C(t_i, t) + \min_{\t \in \segs^{t_i-1}_{i-1}} \loss(\t)  \\ 
		~=~& \min_{t_i} C(t_i, t) + \best^{t_i-1}_{i-1} \\
\end{align*}
	%On page 7: t_i should range from t-W to t-w. It is about the position of the /last/ song.
	
	In this formula $t_i$ ranges from $t-W$ to $t-w$. We have $T \times m$ values of $\best^T_m$ and calculating each takes at most $O(W)$ steps. The total time complexity is $O(TWm)$.
	
\section{Confidence Intervals}\label{sec:confidence-intervals}
	
It may be useful for some applications to build a framework to allow confidence intervals for our predicted indices. This may also be useful for meaningful comparison of cost matrices.
	
	\subsection{Posterior Marginal of Song Boundary}
	Fix a learning rate $\eta$, and fix $T$ and $m$. Let
	\[
	P(j,s) ~=~ 
	\frac{\displaystyle
		\sum_{\t \in \segs^T_m : t_j = s} e^{- \eta \loss(\t)}
	}{\displaystyle
	\sum_{\t \in \segs^T_m} e^{- \eta \loss(\t)}
}
\]
That is, $P(j,s)$ is the ``posterior probability'' that song $j$ starts at time $s$.

To compute $P(j,s)$, we need an extended notion of segmentation. We call $\t$ a $m/F: T$ segmentation if
\[
F = t_1 < \ldots < t_m < t_{m+1} = T+1.
\]
Let $\segs^{F : T}_m$ be the set of all $m/F-T$-segmentations. 
%
We have
\begin{multline*}
	\sum_{\t \in \segs^T_m : t_j = s} e^{- \eta \loss(\t)}
	~=~
	\sum_{\substack{\t \in \segs^{s-1}_{j-1},\\ 
			\t'\in \segs^{s : T}_{m-j+1}}} 
	e^{- \eta (\loss(\t) +\loss(\t'))}
	~=~ \\
	\left(\sum_{\t \in \segs^{s-1}_{j-1}} e^{- \eta \loss(\t)}\right)
	\left(\sum_{\t \in \segs^{s : T}_{m-j+1}} e^{- \eta \loss(\t)}\right)
\end{multline*}
%
which upon abbreviating
\begin{align*}
	\head^t_m & ~=~ \sum_{\t \in \segs^{t}_{m}} e^{- \eta \loss(\t)} &
	\tail^f_m & ~=~ \sum_{\t \in \segs^{f : T}_m} e^{- \eta \loss(\t)}
\end{align*}
means that we can write
\[
P(j,s) ~=~ \frac{\head^{s-1}_{j-1} \cdot \tail^{s}_{m-j+1}}{\head^T_m}
.
\]
So it suffices to compute $\head^{t}_{m}$ and $\tail^{t}_{m}$ for all relevant $t$ and $m$. We use
\begin{align*}
	\head^t_1 &~=~ e^{-\eta C(1,t)} &
	\tail^f_1 &~=~ e^{- \eta C(f, T-f+1)}
\end{align*}
and for $m\ge2$
\begin{align*}
	\head^t_m 
	&~=~ 
	\sum_{t_m} \sum_{\t \in \segs^{t_m-1}_{m-1}} e^{- \eta (\loss(\t) + C(t_m, t-t_m+1))} 
	\\
	&~=~ 
	\sum_{t_m} e^{-\eta C(t_m, t-t_m+1)} \sum_{\t \in \segs^{t_m-1}_{m-1}} e^{- \eta \loss(\t)}
	\\
	&~=~ 
	\sum_{t_m} e^{-\eta C(t_m, t-t_m+1)} \head^{t_m-1}_{m-1}
	\\
	\tail^f_m 
	&~=~ 
	\sum_{t_2} \sum_{\t \in \segs^{t_2 : T}_{m-1}} e^{- \eta (C(f, t_2-f)
		+ \loss(\t))}\\
	&~=~
	\sum_{t_2} e^{- \eta C(f, t_2-f)} \sum_{\t \in \segs^{t_2 : T}_{m-1}} e^{- \eta \loss(\t)}\\
	&~=~
	\sum_{t_2} e^{- \eta C(f, t_2-f)} \tail^{t_2}_{m-1}\\
\end{align*}

\begin{figure}
	\centering
	\includegraphics[width=0.55\textwidth]{images/posterior}
	\caption{A visualization of $log(P(j,s))$ ($\eta=10$) for one of the shows in the test set using the cost matrix parameters from experiment $10$. The actual tracks are overlaid as white crosses.}
	\label{fig:posterior}
\end{figure} 

See Figure~\ref{fig:posterior} for an example of the posterior for a radio show. 

\subsection{Posterior Marginal of Song Position}
Fix a learning rate $\eta$, and fix $T$ and $m$. Let
\[
P(j,s,f) ~=~ 
\frac{\displaystyle
	\sum_{\t \in \segs^T_m : t_j = s \land t_{j+1}-1 = f} e^{- \eta \loss(\t)}
}{\displaystyle
\sum_{\t \in \segs^T_m} e^{- \eta \loss(\t)}
}
\]
That is, $P(j,s,f)$ is the ``posterior probability'' that song $j$ starts at time $s$ and finishes at time $f$.
%
In the same vein as the last section, we now get
\[
P(j,s,f) ~=~ \frac{\head^{s-1}_{j-1} \cdot e^{-\eta C(s, f-s+1)} \cdot \tail^{f+1}_{m-j}}{\head^T_m}.
\]


\subsection{Track Index Confidence}

We can use the posterior marginal of song boundary to give estimates of confidence on track index placement and time accuracy.

To estimate the uncertainty of correct track alignment, we select the next highest probability of other track placements at the same time of the optimal placement from $P(j,s)$ and normalize them by the probability of the optimal placement.

Let track index confidence
\begin{align*}
I( j ) = 1-\frac{ P( \{1,\ldots,M\} \setminus j, \mathrm{SortInd}( P(j, 1,\ldots,T)_1)^2 }{ \max( P(j, 1,\ldots,T)_2) }
\end{align*}

where $\mathrm{SortInd}(l)$ will return the original indices corresponding to the sorted list of $l$. 

\subsection{Track Time Confidence}

Track time uncertainty is estimated by normalizing the value of the next most significant peak in $P(j,\forall s)$ by the probability of actual track placement (which is the most significant peak). 
Let 
\begin{align*}
\tilde I( j ) = 1-\frac{ \mathrm{Peaks}( P(j, 1,\ldots,T ) )_2 } { \mathrm{Peaks}( P(j, 1,\ldots,T ))_1 }
\end{align*} where $Peaks(s_i)$ is a peak finding algorithm that returns the peaks in descending order of magnitude (we used the \texttt{findpeaks} function in MatLab).

See Figure~\ref{fig:overallconfs} for an illustration of $\tilde I( j )$ and $I( j )$.

\section{Methodology}\label{sec:methodology}

We selected $6$ shows at random (two of each show type) to use to develop the cost matrices and to find an optimal set of parameters using a random search. These shows were A State of Trance $453$ and $462$, Magic Island $98$ and $112$ and Trance Around The World $364$ and $372$.  

In our experiments we decided to fix the tile size at $5$ seconds for the sake of speed. A lower tile size does increase accuracy but not significantly. Higher tile sizes can perform more robustly (fewer catastrophic misalignments) but progressively lose out on accuracy.

In our results we will perform some obvious combinations of cost matrices. We also performed a random search of cost matrix contributions and parameters. This has also been provided in the results (see $\dagger$, experiment $12$ in the results). We selected the best set of parameters based on the lowest cost sum of heuristic precision and mean average precision $\sum \hat A(P,A) + \hat H(P,A) \rightarrow \min_i$ for all shows. 

\section{Materials}\label{sec:materials}

All the code presented in this paper with the small working test set is available on GitHub \footnote{\url{github.com/ecsplendid/DanceMusicSegmentation}}. The large data set ($\approx100$GB) we received from Denis Goncharov can easily be made available on request (it is in a Google Drive account and easily shareable).

\section{Results}\label{sec:results}

Please see Table~\ref{tab:results} for the main table of results, Figure~\ref{fig:shifthistogram} for a histogram of predicted versus actual differences for experiment $10$. Figure~\ref{fig:overallconfs} shows a visualization of the confidences and relative performance in relation to show progression (which is a normalized quantity because shows are of variable length).

	\begin{figure}
		\centering
		\includegraphics[width=0.4\textwidth]{images/shift_histogram}
		\caption{Histogram of the differences between reconstructed and human captured time indices on experiment 10. The slight bumps are uncertainty about track placement perhaps related to the $\approx 14.8$ second bar intervals in Trance Music. The humans are much more likely to place an index before a predicted index.  }
		\label{fig:shifthistogram}
	\end{figure} 

	\begin{figure*}[t]
		\centering
		\includegraphics[width=0.45\textwidth]{images/confs_mix}
		\includegraphics[width=0.45\textwidth]{images/confs_sum}
		\caption{The relative performance and confidence ($\eta=10$) of two cost matrices. The left is experiment $10$ which has a mixture of cost matrices (see the main Results table) and the right is experiment $5$ which is only the summation matrix. Ostensibly the summation cost matrix struggles towards the middle of the shows.}
		\label{fig:overallconfs}
	\end{figure*} 

Our parameter search was slightly limited (we searched $2000$ random parameters), and should be done again more extensively. However it was clear from the search that some asymptotic limit was reached. 

Adding the Gaussian cost matrix in most cases improves performance because it regularises the track index estimates (see experiment $6$ and the improvement in $11$). Using the Gaussian cost matrix on its own makes a useful comparison to how our method in general improves on a potential naive approach to the problem. 

The symmetry sum is the best performing single cost matrix with an even incentive bias with summation and evolution close behind. The gains from decreasing window size past $T=5$ asymptotically decay which leads us to believe we are approaching some kind of limit. More interesting is that when we perform a random parameter search we also hit a barrier when we would have expected further improved results. We estimate that we are now limited by the error variable associated with the human indices. The `human disagreement' variable (see Section~\ref{human_acc}) has a standard deviation of $9.13$ seconds which we believe is limiting our progress. 

On our previous work we were using a disincentive only summation matrix, and found that normalizing it on the square root of the width produced the best result. This would have been necessary to encourage placement of longer tracks as no incentive was present. So experiment $11$ is roughly comparable and indeed produces the same overall mean average to that previous experiment ($\approx 20$S). Note that we no longer discard any shows from evaluation which makes the result stronger. 

Figure~\ref{fig:overallconfs} seems to suggest that with the summation matrix alone (and we suspect all of the matrices when used in isolation), we suffered significant loss in confidence and performance in the middle of the shows. This would imply that our method was failing to some extent, and to make matters worse; the magnitude of failure depended on the number of tracks. Apparently the mixture of cost functions removes this effect.

\begin{table*}[t]
	\centering
	\caption{Main results. Tile size $T$ was $5$s for all results. Shift refers to the mean average of the differences in predictions versus human captured indices. $\star$; $h=1662$Hz, $b=5$Hz, $\hat c=1$, $\rho=20$ $\dagger (random search);$ $h=1662$Hz, $b=5$Hz, $\hat c=1.2$, $\rho=26$. Blank cells indicate no contribution from that cost matrix.  }
	\begin{tabular}{|l|l|llllllll|lll|llllll|}
		\hline 
	\begin{sideways}Experiment \end{sideways} &	\begin{sideways}$h$, $l$, $b$, $\hat c$, $\rho$ \end{sideways} &
		\begin{sideways}Sym Sum\end{sideways}&\begin{sideways}Sym Diff\end{sideways} &\begin{sideways}Symmetry\end{sideways}&\begin{sideways}Contig Past\end{sideways} &\begin{sideways}Contig Fut.\end{sideways}&\begin{sideways}Summation\end{sideways}&\begin{sideways}Gaussian\end{sideways} &\begin{sideways}Evolution\end{sideways} & 
	 \begin{sideways}Mean (S)\end{sideways} & \begin{sideways}Heuristic (S)\end{sideways} & \begin{sideways}Shift (S)\end{sideways} & \begin{sideways}60s(\%)\end{sideways} & \begin{sideways}30s(\%)\end{sideways} & \begin{sideways}20s(\%)\end{sideways} & \begin{sideways}10s(\%)\end{sideways} & \begin{sideways}5s(\%)\end{sideways} & \begin{sideways}1s(\%)\end{sideways}    \\ \hline 
	
		
$1$ &	$\ast $	&  \begin{sideways}$1.0_{\Omega^{\frac{1}{2}}}$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways} &\begin{sideways}$$\end{sideways}&  $32.13 $&$ 8.59 $& $ 1.37 $& \begin{sideways}$96.06$\end{sideways}& \begin{sideways}$87.28$\end{sideways}&\begin{sideways}$ 78.15$\end{sideways}&\begin{sideways}$ 60.48$\end{sideways}& \begin{sideways}$33.82$ \end{sideways}& \begin{sideways}$20.89$\end{sideways}  \\
		$2$ &	$\ast $	&  \begin{sideways}$$\end{sideways}& \begin{sideways}$1.0_{\Omega^{\frac{1}{2}}}$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways} &\begin{sideways}$$\end{sideways}&  $35.20 $&$ 16.56$& $ 2.52 $& \begin{sideways}$91.83$\end{sideways}& \begin{sideways}$73.00$\end{sideways}&\begin{sideways}$ 58.54$\end{sideways}&\begin{sideways}$ 36.40$\end{sideways}& \begin{sideways}$19.75$ \end{sideways}& \begin{sideways}$12.41$\end{sideways} \\
	$3$ &	$\ast $	&  \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$1.0_{\Omega^{\frac{1}{2}}}$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways} &\begin{sideways}$$\end{sideways}&  $68.33 $&$ 11.50$& $ 1.39 $& \begin{sideways}$92.70$\end{sideways}& \begin{sideways}$80.35$\end{sideways}&\begin{sideways}$ 69.51$\end{sideways}&\begin{sideways}$ 50.59$\end{sideways}& \begin{sideways}$27.55$ \end{sideways}& \begin{sideways}$17.62$\end{sideways} \\
		$4$ &	$\ast $	&  \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$1.0_{\Omega^{\frac{1}{2}}}$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways} &\begin{sideways}$$\end{sideways}&  $152.58$&$ 21.75$& $ 5.36 $& \begin{sideways}$79.64$\end{sideways}& \begin{sideways}$63.05$\end{sideways}&\begin{sideways}$ 49.20$\end{sideways}&\begin{sideways}$ 33.12$\end{sideways}& \begin{sideways}$17.93$ \end{sideways}& \begin{sideways}$10.90$\end{sideways} \\
	$5$ &	$\ast $	&  \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$1.0_{\Omega^{\frac{1}{2}}}$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways} &\begin{sideways}$$\end{sideways}&  $160.09$&$ 27.60$& $ 12.25$& \begin{sideways}$76.97$\end{sideways}& \begin{sideways}$54.36$\end{sideways}&\begin{sideways}$ 42.27$\end{sideways}&\begin{sideways}$ 27.43$\end{sideways}& \begin{sideways}$14.67$ \end{sideways}& \begin{sideways}$9.04 $\end{sideways}\\
	$6$ &	$\ast $		&  \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$1.0_{\Omega^{\frac{1}{2}}}$\end{sideways}& \begin{sideways}$$\end{sideways} &\begin{sideways}$$\end{sideways}&  $34.30 $&$ 8.65 $& $ 1.52 $& \begin{sideways}$95.85$\end{sideways}& \begin{sideways}$87.17$\end{sideways}&\begin{sideways}$ 77.76$\end{sideways}&\begin{sideways}$ 60.48$\end{sideways}& \begin{sideways}$33.66$ \end{sideways}& \begin{sideways}$20.81$\end{sideways} \\
	$7$ &	$\ast $	&  \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$1.0_{\Omega^{\frac{1}{2}}}$\end{sideways} &\begin{sideways}$$\end{sideways}&  $97.83 $&$ 73.16$& $ -6.08$& \begin{sideways}$43.37$\end{sideways}& \begin{sideways}$22.89$\end{sideways}&\begin{sideways}$ 15.53$\end{sideways}&\begin{sideways}$ 7.54 $\end{sideways}& \begin{sideways}$3.86 $ \end{sideways}& \begin{sideways}$2.38 $\end{sideways}\\
	$8$ &	$\ast $		&  \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways} &\begin{sideways}$1.0_{\Omega^{\frac{1}{2}}}$\end{sideways}&  $33.99 $&$ 8.49 $& $ 3.95 $& \begin{sideways}$96.01$\end{sideways}& \begin{sideways}$87.76$\end{sideways}&\begin{sideways}$ 79.61$\end{sideways}&\begin{sideways}$ 60.70$\end{sideways}& \begin{sideways}$33.82$ \end{sideways}& \begin{sideways}$21.40$\end{sideways} \\
	$9$ &	$\ast $	&  \begin{sideways}$1.0_{\Omega^{\frac{1}{2}}}$\end{sideways}& \begin{sideways}$1.0_{\Omega^{\frac{1}{2}}}$\end{sideways}& \begin{sideways}$1.0_{\Omega^{\frac{1}{2}}}$\end{sideways}& \begin{sideways}$1.0_{\Omega^{\frac{1}{2}}}$\end{sideways}& \begin{sideways}$1.0_{\Omega^{\frac{1}{2}}}$\end{sideways}& \begin{sideways}$1.0_{\Omega^{\frac{1}{2}}}$\end{sideways}& \begin{sideways}$1.0_{\Omega^{\frac{1}{2}}}$\end{sideways} &\begin{sideways}$1.0_{\Omega^{\frac{1}{2}}}$\end{sideways}&  $24.88 $&$ 8.43 $& $ 3.01 $& \begin{sideways}$97.20$\end{sideways}& \begin{sideways}$89.00$\end{sideways}&\begin{sideways}$ 80.24$\end{sideways}&\begin{sideways}$ 60.55$\end{sideways}& \begin{sideways}$33.57$ \end{sideways}& \begin{sideways}$20.98$\end{sideways} \\
	$10$ &	$\ast $	&  \begin{sideways}$1.0_{\Omega^{\frac{1}{2}}}$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$1.0_{\Omega^{\frac{1}{2}}}$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$1.0_{\Omega^{\frac{1}{2}}}$\end{sideways}& \begin{sideways}$1.0_{\Omega^{\frac{1}{2}}}$\end{sideways} &\begin{sideways}$1.0_{\Omega^{\frac{1}{2}}}$\end{sideways}&  $16.31 $&$ 8.09 $& $ 2.40 $& \begin{sideways}$97.61$\end{sideways}& \begin{sideways}$89.23$\end{sideways}&\begin{sideways}$ 80.93$\end{sideways}&\begin{sideways}$ 61.87$\end{sideways}& \begin{sideways}$34.85$ \end{sideways}& \begin{sideways}$22.07$\end{sideways} \\ 
		
	$11$ &	$\ast $	&  \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$$\end{sideways}& \begin{sideways}$1.0_{\Omega^{\frac{1}{2}}}$\end{sideways}& \begin{sideways}$1.0_{\Omega^{0.8}}$\end{sideways} &\begin{sideways}$$\end{sideways}&  $21.57 $&$ 8.92$& $ 1.52$& \begin{sideways}$97.19$\end{sideways}& \begin{sideways}$87.57$\end{sideways}&\begin{sideways}$ 78.19$\end{sideways}&\begin{sideways}$ 59.34 $\end{sideways}& \begin{sideways}$32.65 $ \end{sideways}& \begin{sideways}$20.64 $\end{sideways}\\
		
		\hline 
$12$ &	$\dagger$	&\begin{sideways}$0.4_{\Omega^{0.4}}$\end{sideways}&\begin{sideways}$0.1_{\Omega^{0.1}}$\end{sideways}&\begin{sideways}$0.1_{\Omega^{0.7}}$\end{sideways}&\begin{sideways}$0.2_{\Omega^{0.2}}$\end{sideways}&\begin{sideways}$0.0_{\Omega^{0.7}}$\end{sideways}&\begin{sideways}$0.6_{\Omega^{0.6}}$\end{sideways}&\begin{sideways}$0.7_{\Omega^{0.8}}$\end{sideways}&\begin{sideways}$0.4_{\Omega^{0.1}}$\end{sideways}&$16.91 $&$ 8.27 $& $ 2.33 $& \begin{sideways}$97.93$\end{sideways}& \begin{sideways}$89.76$\end{sideways}&\begin{sideways}$ 80.74$\end{sideways}&\begin{sideways}$ 61.22$\end{sideways}& \begin{sideways}$34.65$ \end{sideways}& \begin{sideways}$22.16$\end{sideways} \\
		
		
				\hline 
	\end{tabular}
	
	\label{tab:results}
\end{table*}


\section{Summary}\label{conclusions}

We believe our algorithm would be useful for segmenting DJ-mixed audio streams in batch mode. It would be excellent if Spotify\footnote{\url{http://spotify.com}} for example started to do something similar. Spotify is an on-line music service with many electronic dance music radio shows with the track listing in text. This method would allow them to reliably segment the shows, and they could display an interactive segmentation in the music player. 

The new cost matrices in combination improve robustness significantly over single matrices or regularised single matrices (as in our last paper). We are seeing about a 50\% improvement in overall mean accuracy over single cost matrices that are correctly normalized. The new cost matrices improve on many drawbacks of our previous work (mainly that it was vulnerable to dissimilar regions within tracks). 

Our method still has one key drawback that we are aware of. This is the rare instance where there are head or tail segments to a track that seem independent from the rest of the track. When these are small they usually get absorbed without any problems but they can cause misplacements. In spite of this issue we suspect that our predictions are more accurate and more consistent than the human equivalents while not being as precise in situations when our index agrees with theirs.

A more precise corpus where the DJ was also the cuesheet author would allow us to tune the parameters more succinctly and also the generation of an artificial dataset to test against. This work is forthcoming. 

We would also like to implement some of the methods in the literature (which were mostly designed for scene analysis) to see if we outperform them. It would be tricky to get an exact comparison because we could not find an unsupervised deterministic algorithm which finds a fixed number of strictly contiguous clusters. We could however adapt existing algorithms to get a like for like comparison. We would like to evaluate the performance of J Theiler's contiguous K-means algorithm in particular \cite{theiler1997contiguity} and also similar algorithms. We have the property of being deterministic but probabilistic methods should be explored. Theiler's algorithm would require some modification to work in this scenario because we require strictly contiguous clusters, not just a contiguity bias. 

\section{Acknowledgements}\label{sec:acknowledgements}

We would like to thank Mikael Lindgren and Denis Goncharov from cuenation\footnote{\url{http://www.cuenation.com}} for their help explaining how they make cue-sheets and for providing the data set to test the algorithm on.

\bibliographystyle{ieeetr}
\bibliography{bib/references,bib/refs}

\end{document}
